## 0 写在前面

参考资料：Statistical Learning Theory- Models, Concepts, and Results
最近需要做一个统计学习理论的总结，笔者主要参考上面资料做了前面5章的总结，特此分享 :)。

## 1 引言

统计学习理论(SLT)为当代很多机器学习算法提供理论基础，也是AI最美的发展分支之一。它起源于1960年代的俄罗斯，并且在1990年代，当SVM成为从计算机视觉到计算生物学等主要领域中模式识别的的一个标准工具之后，SLT开始流行起来。

本篇主要展示一个非技术性综述，读者不需要深入的数学统计、计算机背景。现已有很多杰出的文献总结SLT，Vapnik的专著(1995，1998)中是SLT的基石之一，。。。

## 2 SLT的标准框架

### 2.1 背景

在我们文章中，学习指的是从观察案例中推导出的一般性规律。比如孩子们能够通过观察一些物体实例来知道车是什么，而不需要知道如何制造汽车的规则，仅仅是观察。

机器学习不是学习生命体学习的过程，而是从抽象上学习这个学习的过程。问题在于机器或者说计算机如何能够通过一些特定的学习算法来"学习"执行一些特定的任务。

常见的问题有分类、回归和聚类，机器学习针对这些问题有不同的焦点，其中之一便是归纳泛化能力。

目前研究最透彻的是分类。这里我们需要处理两种空间：输入空间$\mathcal X$和输出空间$\mathcal Y$。如果需要将给定对象分成有限类，那么$\mathcal X$就是所有可能对象(实例)的空间，$\mathcal Y$就是包含所有分类的空间，学习算法使得给定一些训练实例如$\left(X_{1}, Y_{1}\right), \ldots,\left(X_{n}, Y_{n}\right)$，然后贴上不同的分类标签。我们的目标就是找到这样一个映射(mapping)$f: X \rightarrow Y$ ，使得分类的错误率越来越小，这样的映射就是分类器。

SLT是机器学习的理论分支，以下是SLT中的几个基本问题：

* 什么样的学习任务能够由计算机完成？

* 机器学习需要哪些假设条件？

* 学习算法成功的关键属性是什么？

* 对于特定的学习算法我们能保证有怎样的表现？

我们首先关注监督学习，尤其是典型的二分类问题，其他学习问题的理论仍然处于幼年时期。

### 2.2 假设条件

首先我们需要假设 输入输出空间$\mathcal X \times \mathcal Y$ 之间存在联合分布概率(joint probability distribution)$P$，训练样本$\left(X_{i}, Y_{i}\right)$ 独立于该分布$P$进行采样，这种类型的采样通常称为iid采样(独立同分布)。以下两点假设比较重要：

* 没有关于P的假设。在标准SLT中，我们没有对概率分布P进行任何假设：它可以是上的任何分布。从这个意义上说，统计学习理论的不可知性(agostic setting)不同于标准统计，从标准统计中，通常会假设概率分布属于某个分布族，目标是估计该分布的参数。

* 没有因为标签噪声或类别重叠导致的不确定标签。请注意，P不仅是实例X的概率分布，而且是标签Y的概率分布。因此，数据中的标签Yi不一定只是对象Xi的确定性函数，它们本身可以是随机的。出现这种情况的主要原因有两个。第一个原因是数据生成过程可能会受到标签噪声的影响。也就是说，在学习过程中作为培训标签获得的标签Yi可能是错误的。这是一个重要而现实的假设。例如，为了生成用于检测电子邮件垃圾邮件的训练数据，需要人工将电子邮件手工标记为“垃圾邮件”和“非垃圾邮件”类别。所有人类都会不时犯错。因此，即使某些电子邮件不是垃圾邮件，也有可能偶然将其标记为“垃圾邮件”，反之亦然。当然，希望这样的错误标签仅以相对较小的概率出现。导致非确定性标签的第二个主要原因是重叠类的情况。例如，考虑根据身高预测一个人的性别的任务。显然，身高1.80米的人原则上可以是男性或女性，因此我们不能为输入X = 1.80分配唯一的标签Y。

其他的一些假设：

* 独立采样

* 分布$P$是固定的

* 分布$P$在学习过程中未知

#### 2.2.1 损失函数和风险函数

损失函数，又叫代价函数，是将随机事件或其有关随机变量的取值映射为非负实数以表示该随机事件的“风险”或“损失”的函数。简单的0-1分类损失函数如下：
$$
\ell(X, Y, f(X))=\left\{\begin{array}{ll}{1} & {\text { if } f(X) \neq Y} \\ {0} & {\text { otherwise }}\end{array}\right.
$$
还有更常见的方差损失函数：$\ell(X, Y, f(X))=(Y-f(X))^{2}$ 。

基于损失函数，风险函数被定义为所有数据点的平均损失：
$$
R(f):=E(\ell(X, Y, f(X)))
$$
如果风险越小，那么分类器就越好。

#### 2.2.2 贝叶斯风险

我们考虑贝叶斯分类器(它是所有可测函数中最优的)，对于给定分布$P$，如下：
$$
f_{\text {Bayes}}(x):=\left\{\begin{array}{ll}{1} & {\text { if } P(Y=1 | X=x) \geq 0.5} \\ {-1} & {\text { otherwise. }}\end{array}\right.
$$

但是基本不可能直接计算出贝叶斯分类，因为对于学习者来说概率分布P不可知，并且同样因此也不能计算风险函数$R(f)$，这时候就需要用到SLT了。

### 2.3 泛化与相合性(一致性)

#### 2.3.1 泛化(genelization)

尽管不能计算真实的风险函数，但是可以计算训练集中的错误个数，称为经验风险或者训练误差：
$$
 \mathrm{R}_{\mathrm{emp}}(f):=\frac{1}{n} \sum_{i=1}^{n} \ell\left(X_{i}, Y_{i}, f\left(X_{i}\right)\right)
$$
通常对于特定的训练样本，观察风险相对较小。然而，分类器$f_{n}$在训练集中产生一些错误后会不会对剩下的样本空间产生误差还不清楚。一个好的分类器中经验风险都会接近于真实风险，即如果$\left|R\left(f_{n}\right)-\mathrm{R}_{\mathrm{emp}}\left(f_{n}\right)\right|$很小，我们称一个分类器$f_{n}$能够泛化得很好。但是通常经验误差会小于真实误差，而且有些情况甚至差距很大。

#### 2.3.2 bias/variance tradeoff(误差/方差权衡)

#### 2.3.2 相合性(consistency)

对于给定的函数或者分类器空间$\mathcal F$ ，如果一般分类器$f_{n} \in \mathcal{F}$，可以认为这些分类器是最好的分类器，记作$f_{\mathcal{F}}$，换句话说它们的风险最小，即：
$$
f_{\mathcal{F}}=\underset{f \in \mathcal{F}}{\operatorname{argmin}} R(f)
$$
第三种分类器就是贝叶斯分类器$f_{B a y e s}$ ，即认为最好的分类器，根据这三种分类器可以定义三种相合性：

* 类$\mathcal{F}$相合

  即对于所有$\varepsilon>0$，有$P\left(R\left(f_{n}\right)-R\left(f_{\mathcal{F}}\right)>\varepsilon\right) \rightarrow 0 \text { as } n \rightarrow \infty$

* 贝叶斯相合

  即对于所有$\varepsilon>0$，有$P\left(R\left(f_{n}\right)-R\left(f_{\text {Bayes }}\right)>\varepsilon\right) \rightarrow 0 \text { as } n \rightarrow \infty$

* 普遍相合

  对于所有分布$P$都满足贝叶斯相合性

以上的相合性称为弱收敛，即以概率收敛，而强收敛即几乎必然收敛。

### 2.4  trade-off(权衡)

#### 2.4.1 bias/variance tradeoff(误差/方差权衡)

比如对于回归问题，我们通常会讨论时用线性函数$y=\theta_{0}+\theta_{1} x$去拟合，还是用更复杂的多项式函数$
y=\theta_{0}+\theta_{1} x+\cdots \theta_{5} x^{5}
$拟合，如下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191203125053200.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pvaG5KaW0w,size_16,color_FFFFFF,t_70)

尽管右图的复杂模型能够很好的拟合训练数据，但是应用到测试数据上可能会差，即泛化不一定好，发生过拟合，而简单如左图的缺点就是系统偏差bias较大，可能会欠拟合。泛化的另外一个问题就是训练数据的随机性，即方差variance，所以就更没有必要在训练集中追求完美。综上，在偏差方差之间会有一个权衡的过程。

#### 2.4.2 estimation-approximation trade-off(估计/逼近权衡)

对于贝叶斯相合性，可以拆解为以下两个部分：
$$
R\left(f_{n}\right)-R\left(f_{B a y e s}\right)=\underbrace{\left(R\left(f_{n}\right)-R\left(f_{\mathcal{F}}\right)\right)}_{\text {estimation error }}+\underbrace{\left(R\left(f_{\mathcal{F}}\right)-R\left(f_{\text {Bayes}}\right)\right)}_{\text {approximation error }}
$$
如下图，一个是估计误差，一个是逼近误差。估计误差设计随机采样的不确定性，即对于给定样本我们需要找到最佳函数空间$\mathcal{F}$这个过程中产生的误差。逼近误差跟随机样本无关，它是在小空间$\mathcal{F}$的基础上再找到最佳的函数产生的，用于衡量这些函数近似$\mathcal{F}_{all}$效果的量。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191203125106392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pvaG5KaW0w,size_16,color_FFFFFF,t_70)

在统计学中，估计误差也叫方差(variance)，逼近误差也叫偏差(bias)。下图描述了两个误差之间随着函数复杂度的权衡，可以看出最佳的风险值在中等复杂度处。在SLT中，重点关注估计错误，逼近错误的分析较为困难。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191203125114841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pvaG5KaW0w,size_16,color_FFFFFF,t_70)

## 3 kNN分类器的泛化与相合性

第一个被严格证明具有普遍相合性的分类器是在1977年有Stone完成，这个分类器就是kNN。kNN是最简单、应用最广泛的分类器之一。Stone给出了如下定理：

* **贝叶斯普遍相合性**  $f_{n}$为对于$n$个样本点的k邻近分类器，如果$n \rightarrow \infty$，$k \rightarrow \infty$并且$k / n \rightarrow 0$，那么对于所有概率分布$P$，就有$R\left(f_{n}\right) \rightarrow R\left(f_{\text {Bayes }}\right)$

这个定理告诉我们如果选择增长较为缓慢的参数$k$，即满足$k / n \rightarrow 0$比如$k \approx \log (n)$，那么kNN就具有普遍相合性，注意这是对大样本极限情况而言，对于有限或者小样本，kNN并不算好。

## 4 经验风险最小化

前面提到经验风险表达式如下：
$$
R(f)=E(\ell(X, Y, f(X))
$$
但是由于分布$P$未知，所以我们不能计算$R(f)$。但是我们可以通过已知训练样本推出一个分布$P$和函数$f$，使其风险达到最佳，这种思路应用了归纳原理。通常用经验风险$R_{\mathrm{emp}}(f)$代替$R(f)$：
$$
R_{\mathrm{emp}}(f)=\frac{1}{n} \sum_{i=1}^{n} \ell\left(X_{i}, Y_{i}, f\left(X_{i}\right)\right.
$$
这样我们需要找到一个函数或者说分类器使得经验风险最小：
$$
f_{n}:=\underset{f \in \mathcal{F}}{\operatorname{argmin}} \mathrm{R}_{\mathrm{emp}}(f)
$$

### 4.1 大数定律

大数定律是统计学中重要理论之一。它的内容是当样本大小趋近于无穷大的时候，满足独立同分布的随机变量$\xi_{i}$的均值收敛于收敛于基础分布(underlying distribution)的均值(或者说期望)：
$$
\frac{1}{n} \sum^{n} \xi_{i} \rightarrow E(\xi) \quad \text { for } n \rightarrow \infty
$$
那么它的经验风险可以收敛于其真实风险$R(f)$：
$$
\mathrm{R}_{\mathrm{emp}}(f)=\frac{1}{n} \sum_{i=1}^{n} \ell\left(X_{i}, Y_{i}, f\left(X_{i}\right)\right) \rightarrow E(\ell(X, Y, f(X))) \quad \text { for } n \rightarrow \infty
$$
即对于给定的有限样本，我们可以用经验风险近似真实风险，霍夫丁不等式(切尔诺夫界)表征了经验风险逼近真实风险的效果。即如果随机变量$\xi_{i}$只在区间$[0,1]$取值，那么有：
$$
P\left(\left|\frac{1}{n} \sum_{i=1}^{n} \xi_{i}-E(\xi)\right| \geq \epsilon\right) \leq 2 \exp \left(-2 n \epsilon^{2}\right)
$$
从该式可以看出当$n$越大，偏差的概率就会越低，应用于经验和真实风险有：
$$
P\left(\left|R_{\mathrm{emp}}(f)-R(f)\right| \geq \epsilon\right) \leq 2 \exp \left(-2 n \epsilon^{2}\right)
$$
上式也就切诺夫边界，它的一个关键特性就是他本质上是基于概率性的。但是注意，切诺夫边界只对固定函数$f$有效，即不依赖于训练函数。然而，分类器$f_{n}$确实依赖于训练数据，因为我们就是用训练数据选择它的。尽管这是一个细微的数学差异，但也很可能是经验风险最小化完全出错的地方。

### 4.2 为什么经验风险最小化可能会不相合？

设数据空间$\mathcal X=[0,1]$，选择$\mathcal X$上的均匀分布，对于一个输入点$X$，定义标签$Y$如下：
$$
Y=\left\{\begin{array}{ll}{-1} & {\text { if } X<0.5} \\ {1} & {\text { if } X \geq 0.5}\end{array}\right.
$$
假设给定训练点$\left(X_{i}, Y_{i}\right)_{i=1, \dots, n}$，考虑如下分类器：
$$
f_{n}(X)=\left\{\begin{array}{ll}{Y_{i}} & {\text { if } X=X_{i} \text { for some } i=1, \ldots, n} \\ {1} & {\text { otherwise. }}\end{array}\right.
$$
该分类器$f_{n}$能够完美分类所有训练点，即经验风险$\mathrm{R}_{\mathrm{emp}}\left(f_{n}\right)=0$。但是该分类器并没有学习任何东西，仅仅是记住了训练标签并且在其他情况下仅仅预测为1，即该分类器将不相合。比如，给定一个测试点$(X, Y)$，通常该测试点不同于任何训练集中的点，在这种情况下它仅输出标签1。如果$X>0.5$，那么这个输出标签正确，但是如果$X<0.5$，那么这就是一个错误标签。可以知道它的真实风险为$R\left(f_{n}\right)=1 / 2$。事实上，这是一个典型的过拟合，即分类器很好地适应了训练数据但是不能从新的测试数据学习到任何东西。所以该分类器不相合。注意到标签是关于输入点的一个确定函数，即可以得到贝叶斯风险为0，于是有$1 / 2=R\left(f_{n}\right) \not \rightarrow R\left(f_{B a y e s}\right)=0$。

### 4.3 一致收敛

经验风险最小化所需的条件包括限定可允许的函数集。VC (Vapnik-Chervonenkis)理论认为经验风险最小化的相合性(一致性)取决于所有函数$f \in \mathcal{F}$的最坏情况行为。上面可以看出这个最坏的情况不是标准大数定律，而是大数定律其中的一个版本，即统一大数定律。下图给出了统一大数定律和一致性问题的简化描述。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191203125124315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pvaG5KaW0w,size_16,color_FFFFFF,t_70)

 $y$轴表示风险。 对于每个固定函数$f$，大数定律告诉我们，随着样本量达到无穷大，经验风险$R_{\mathrm{emp}}(f)$向真实风险$R(f)$收敛（如箭头所示）。 但是，这并不意味着在无限样本量的限制内，经验风险的最小值fn会导致该风险的值与函数类别中最佳函数$f_{F}$的风险一样好(在图中用$f_{F}$表示)。 为了使后者正确，我们要求$R_{\mathrm{emp}}(f)$向$R(f)$的收敛在F中的所有函数上都是统一的(Scholkopf和Smola，2002)。

因此一种确保$ \mathcal F$中所有函数的最小值收敛的一种方法是在 $\mathcal F$ 上均匀收敛：我们要求所有函数$f \in \mathcal{F}$，真实风险与经验风险之差必须同时变小。也就是说，我们要求存在足够大的$n$，并且样本大小至少大于$n$，这样对于所有$f \in \mathcal{F}$，都会有$\left|R(f)-\mathrm{R}_{\mathrm{emp}}(f)\right|$小于一个给定值$\varepsilon$，我们使用一个上确界(supremum)来表达：
$$
\sup _{f \in \mathcal{F}}\left|R(f)-\operatorname{R}_{\mathrm{emp}}(f)\right| \leq \varepsilon
$$
因此，有：
$$
\left|R(f)-\mathrm{R}_{\mathrm{emp}}(f)\right| \leq \sup _{f \in \mathcal{F}}\left|R(f)-\mathrm{R}_{\mathrm{emp}}(f)\right|
$$
特别地，该关系对于基于有限训练样本的函数$f_{n}$也成立，即推出：
$$
P\left(\left|R\left(f_{n}\right)-\mathrm{R}_{\mathrm{emp}}\left(f_{n}\right)\right| \geq \varepsilon\right) \leq P\left(\sup _{f \in \mathcal{F}}\left|R(f)-\mathrm{R}_{\mathrm{emp}}(f)\right| \geq \varepsilon\right)
$$
式子右边跟统一大数定律(uniform law of large numbers)有关，即当所有$\varepsilon >0$时，大数定律对于函数类$\mathcal{F}$具有一致性，
$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-\mathrm{R}_{\mathrm{emp}}(f)\right| \geq \varepsilon\right) \rightarrow 0 \text { as } n \rightarrow \infty
$$
证明过程如下，
$$
\begin{array}{l}{\left|R\left(f_{n}\right)-R\left(f_{\mathcal{F}}\right)\right|} \\ {\left.\quad \text { (by definition of } f_{\mathcal{F}} \text { we know that } R\left(f_{n}\right)-R\left(f_{\mathcal{F}}\right) \geq 0\right)} \\ {=R\left(f_{n}\right)-R\left(f_{\mathcal{F}}\right)} \\ {=R\left(f_{n}\right)-\operatorname{R}_{\mathrm{emp}}\left(f_{n}\right)+\operatorname{R}_{\mathrm{emp}}\left(f_{n}\right)-\operatorname{R}_{\mathrm{emp}}\left(f_{\mathcal{F}}\right)+\mathrm{R}_{\mathrm{emp}}\left(f_{\mathcal{F}}\right)-R\left(f_{\mathcal{F}}\right)} \\ {\quad\left(\text {note that } \mathrm{R}_{\mathrm{emp}}\left(f_{n}\right)-\mathrm{R}_{\mathrm{emp}}\left(f_{\mathcal{F}}\right) \leq 0 \text { by def. of } f_{n}\right)} \\ {\leq R\left(f_{n}\right)-\mathrm{R}_{\mathrm{emp}}\left(f_{n}\right)+\mathrm{R}_{\mathrm{emp}}\left(f_{\mathcal{F}}\right)-R\left(f_{\mathcal{F}}\right)} \\ {\leq 2 \sup _{f \in \mathcal{F}}\left|R(f)-\mathrm{R}_{\mathrm{emp}}(f)\right|}\end{array}
$$
即：
$$
P\left(\left|R\left(f_{n}\right)-R\left(f_{\mathcal{F}}\right)\right| \geq \varepsilon\right) \leq P\left(\sup _{f \in \mathcal{F}}\left|R(f)-\mathrm{R}_{\mathrm{emp}}(f)\right| \geq \varepsilon / 2\right)
$$
在大数定律下，右边趋近于0，这将导致经验风险(ERM)相对于函数类(underlying function class)$\mathcal{F}$

具有相合性，也就是说，$\mathcal{F}$的一致收敛导致是其经验风险最小化的充要条件。那么是不是必要条件呢？

VC((Vapnik & Chervonenkis))定理3给出了证明，即一致收敛条件：
$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|>\epsilon\right) \rightarrow 0 \text { as } n \rightarrow \infty
$$
对于所有$\varepsilon >0$，该条件是经验风险最小化相合性的充要条件，具体过程略。

## 5 容量概念与泛化界(Capacity concepts and generalization bounds)

上面讨论了一致收敛和经验风险最小化的关系，下面将看到对于如下概率更为广泛的情况：
$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|>\epsilon\right)
$$
在此过程中需要用到两个技巧：并界和影子样本对称化。

### 5.1 并界(union bound)

并界是一个将单个函数的标准大数定律转换为有限函数集的大数定律的简便工具。设集合$\mathcal F$包含有限函数，即$\mathcal{F}=\left\{f_{1}, f_{2}, \ldots, f_{m}\right\}$。那么对于每个函数$f_{i} \in \mathcal{F}$都满足切尔诺夫界形式下的大数定律，即：
$$
P\left(\left|R\left(f_{i}\right)-\mathrm{R}_{\mathrm{emp}}\left(f_{i}\right)\right| \geq \varepsilon\right) \leq 2 \exp \left(-2 n \varepsilon^{2}\right)
$$
现在将个体函数$f_{i}$转换到统一大数定律(uniform law of large numbers)下，即：
$$
\begin{array}{l}{P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right| \geq \varepsilon\right)} \\ {=P\left(\left|R\left(f_{1}\right)-R_{\mathrm{emp}}\left(f_{1}\right)\right| \geq \varepsilon \text { or }\left|R\left(f_{2}\right)-R_{\mathrm{emp}}\left(f_{2}\right)\right| \geq \varepsilon \text { or } \ldots \text { or }\left|R\left(f_{m}\right)-R_{\mathrm{emp}}\left(f_{m}\right)\right| \geq \varepsilon\right)} \\ {\leq \sum_{i=1}^{m} P\left(\left|R\left(f_{i}\right)-R_{\mathrm{emp}}\left(f_{i}\right)\right| \geq \varepsilon\right)} \\ {\leq 2 m \exp \left(-2 n \varepsilon^{2}\right)}\end{array}
$$
第一个等式利用了上确界的定义，注意是"or", 第二个则用到概率论中的一个标准工具，也就是本部分所讲的并界。最后仍然可以简化为类似于如下的个体函数切尔诺夫界：
$$
P\left(\left|R_{\mathrm{emp}}(f)-R(f)\right| \geq \epsilon\right) \leq 2 \exp \left(-2 n \epsilon^{2}\right)
$$
可以看到，切尔诺夫界对于单个函数和函数集的差别在于有个因子$m$，即函数的个数。但是当$n \rightarrow \infty$时，$2 m \exp \left(-2 n \varepsilon^{2}\right)$仍然趋近于0，也就是仍然满足经验风险最小化的相合性。

### 5.2 对称化

对称化是使用函数类容量度量的重要技术步骤。它的重要目的就是用一个基于给定样本计算的量替换上确界

$\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|$。设给定样本$\left(X_{i}, Y_{i}\right)_{i=1, \ldots, n}$，现在引入一个新样本，称之为影子样本(ghost sample)$\left(X_{i}^{\prime}, Y_{i}^{\prime}\right)_{i=1, \ldots, r}$。影子样本同样也是独立同分布的，并且能够计算出其经验风险为$\mathrm{R}_{\mathrm{emp}}^{\prime}(f)$。由此给出VC定理4，如下：

即对于所有$m \epsilon^{2} \geq 2$，都有：
$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|>\epsilon\right) \leq 2 P\left(\sup _{f \in \mathcal{F}}\left|R_{\mathrm{emp}}(f)-R_{\mathrm{emp}}^{\prime}(f)\right|>\epsilon / 2\right)
$$
第一个P是大小为n的独立同分布(iid)，第二个P则是两个大小为n的iid(原始和影子样本)，也就是大小为2n的iid。

从该定理不难看出，如果两个独立的n样本的经验风险很接近，那么他们应该也接近于真实风险，这个定理又叫对称化定理。这条定理的主要目的就是用能够从有限样本计算得出的$\mathrm{R}_{\mathrm{emp}}^{\prime}(f)$来替换不能计算的$R(f)$。

现在来解释对称化定理能够用在什么地方，前面提到我们能够确定关于有限函数类的一致收敛边界：
$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|>\epsilon\right)
$$
但其实即使$\mathcal{F}$包含无限个函数，但是它分类包含$n$个样本点的训练集的方式却是有限的。例如，对于给定的$n$个训练点$X_{1}, \ldots, X_{n}$，和标签值${-1,+1}$，一个函数最多表现$2^{n}$个不同的方式，即它能给每个$Y_{i}$选定-1或者+1值。考虑如下式子：
$$
\sup _{f \in \mathcal{F}}\left|R_{\mathrm{emp}}(f)-R_{\mathrm{emp}}^{\prime}(f)\right|
$$
由于标签值有限，那么肯定存在两个函数$f, g \in \mathcal{F}$在给定样本上具有相同的输出值和经验风险，即$\mathrm{R}_{\mathrm{emp}}(f)=\mathrm{R}_{\mathrm{emp}}(g)$，同理对于影子样本和$\mathrm{R}_{\mathrm{emp}}^{\prime}(f)$也相同，从而他们的$\left|R_{\mathrm{emp}}(f)-R_{\mathrm{emp}}^{\prime}(f)\right|$也将会相同。

因此我们只需要考虑有效的$2^{2 n}$个函数就行，换句话说可以将无限函数类$\mathcal{F}$替换为包含至多$2^{2 n}$个函数的优先函数类(finite function class)。

### 5.3 粉碎系数(shattering coefficient)

上节讲到一个函数类至多有$2^{2 n}$个有效函数，但实际上一般都小于$2^{2 n}$。比如对于给定大小为$n$的样本，令$Z_{n}:=\left(\left(X_{1}, Y_{1}\right), \ldots,\left(X_{n}, Y_{n}\right)\right)$，我们用$\left|\mathcal{F}_{Z_{n}}\right|$分别表示有限化后的函数类$ \mathcal F$(即包含$2_{n}$个函数)中能够正确区分每个样本点的函数个数，其中最大值$\mathcal{N}(\mathcal{F}, n)$我们称为粉碎系数：
$$
\mathcal{N}(\mathcal{F}, n)=\max \left\{\left|\mathcal{F}_{Z_{n}}\right| | X_{1}, \ldots, X_{n} \in \mathcal{X}\right\}
$$
当$\mathcal{N}(\mathcal{F}, n)=2^{n}$时，称之为完全粉碎，它就是一个函数的容量，用来表示函数类的复杂度和多样性，这里再提醒一下，所说的函数在机器学习中指的就是分类器。

### 5.4 一致收敛边界

在并界这一节中已经推出了包含多个函数的函数类的收敛边界，但是这还不够精确，尤其是对于包含较多函数的函数类，根据对称化得出的结论(不管函数类包含多少个函数，它都可以看作至多包含$2^{n}$个函数的有限函数类)，现在可以用粉碎系数来进一步精确的推出收敛边界：
$$
\begin{array}{l}{P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\text {emp }}(f)\right|>\epsilon\right)} \\ {\text { (due to symmetrization) }} \\ {\leq 2 P\left(\sup _{f \in \mathcal{F}}\left|R_{\text {emp }}(f)-R_{\text {emp }}^{\prime}(f)\right|>\epsilon / 2\right)} \\ {\leq 2 P\left(\sup _{f \in \mathcal{F}}\left|R_{\text {emp }}(f)-R_{\text {emp }}^{\prime}(f)\right|>\epsilon / 2\right)} \\ {\text { (only functions in } \mathcal{F}_{Z_{2 n}} \text { are important) }} \\ {\left.=2 P\left(\sup _{f \in \mathcal{F}_{Z_{2 n}}} | R_{\text {emp }}(f)-R_{\text {emp }}^{\prime}(f)\right) |>\epsilon / 2\right)} \\ {\quad\left(\mathcal{F}_{Z_{2 n}} \text { contains at most } \mathcal{N}(\mathcal{F}, 2 n) \text { functions, independently of } Z_{2 n}\right\rangle} \\ {\text { (use union bound argument and Chernoff) }} \\ {\leq 2 \mathcal{N}(\mathcal{F}, 2 n) \exp \left(-n \varepsilon^{2} / 4\right)}\end{array}
$$
即得出：
$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|>\epsilon\right) \leq 2 \mathcal{N}(\mathcal{F}, 2 n) \exp \left(-n \varepsilon^{2} / 4\right)
$$
那么同样也可以得出当$n \rightarrow \infty$右边式子收敛于0时，经验风险最小化具有相合性这个结论。结合并界那一节，现在可以依次写出单个函数的收敛边界、有限函数的一致收敛边界以及更为有限与无限函数的一致收敛边界了：
$$
P\left(\left|R_{\mathrm{emp}}(f)-R(f)\right| \geq \epsilon\right) \leq 2 \exp \left(-2 n \epsilon^{2}\right)
$$

$$
P\left(\left|R_{\mathrm{emp}}(f)-R(f)\right| \geq \epsilon\right) \leq 2m \exp \left(-2 n \epsilon^{2}\right)
$$

$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|>\epsilon\right) \leq 2 \mathcal{N}(\mathcal{F}, 2 n) \exp \left(-n \varepsilon^{2} / 4\right)
$$

并且式子右边都满足当$n \rightarrow \infty$右边式子收敛于0，也就满足了经验风险最小化相合性的充要条件，如下

VC((Vapnik & Chervonenkis))定理3给出了证明，即一致收敛条件：
$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|>\epsilon\right) \rightarrow 0 \text { as } n \rightarrow \infty
$$
对于所有$\varepsilon >0$，该条件是经验风险最小化相合性的充要条件。

### 5.5 泛化边界

对于上节得出的式子：
$$
P\left(\sup _{f \in \mathcal{F}}\left|R(f)-R_{\mathrm{emp}}(f)\right|>\epsilon\right) \leq 2 \mathcal{N}(\mathcal{F}, 2 n) \exp \left(-n \varepsilon^{2} / 4\right)
$$
与其固定$\epsilon$值来求解概率，不如令其右边式子等于一个$\delta>0$(注意该值收敛于0，即可看作0)，那么首先就可以求出：
$$
\epsilon=\sqrt{\frac{4}{n}(\log (2 \mathcal{N}(\mathcal{F}, n))-\log (\delta))}
$$
对于左边式子，由于其$>\epsilon$的概率为0，那么必然有：
$$
R(f) \leq R_{\mathrm{emp}}(f)+\sqrt{\left.\frac{4}{n}(\log (2 \mathcal N(\mathcal{F}, n))-\log (\delta)\right)}
$$
这对于所有$f \in \mathcal{F}$都成立，一方面这是一个优点，另一方面不是所有函数或者说分类器满足经验风险最小化条件，但是这个边界仍然使用，这也可以理解为是一个缺点。